import torch
import sys, os, argparse
from datetime import datetime
import warnings

warnings.filterwarnings(
    "ignore", category=UserWarning, module="transformers.convert_slow_tokenizer"
)
warnings.filterwarnings("ignore", category=FutureWarning, module="transformers")

# ‡πÉ‡∏´‡πâ import preprocess.py ‡πÑ‡∏î‡πâ (‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô ../src)
sys.path.append(os.path.join(os.path.dirname(os.getcwd()), "src"))

from transformers import (
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    AutoTokenizer,
)
from preprocess import load_thaisum, preprocess_dataset, MODEL_NAME


def _resolve_output_dir(name: str, overwrite: bool) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á path ./model/<name>, ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡πÄ‡∏ï‡∏¥‡∏° timestamp ‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡∏ö (‡πÄ‡∏ß‡πâ‡∏ô‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ --overwrite)"""
    base = os.path.join(".", "model", name)
    if os.path.isdir(base) and not overwrite:
        ts = datetime.now().strftime("%Y-%m-%d-%H%M")
        print(f"‚ö†Ô∏è  Output dir exists: {base} ‚Üí will use timestamped dir instead.")
        base = f"{base}-{ts}"
    return base


def main():
    # ===== Argument parser =====
    parser = argparse.ArgumentParser(description="Train mT5 on ThaiSum dataset")
    parser.add_argument(
        "--size",
        type=float,
        default=1.0,
        help="‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á train set ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (0‚Äì1), ‡πÄ‡∏ä‡πà‡∏ô 0.4 = 40% (default=1.0 ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)",
    )
    parser.add_argument(
        "--name",
        type=str,
        default="FineTuned-mT5-ThaiSum",
        help="‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏à‡∏∞‡πÄ‡∏ã‡∏ü‡∏ó‡∏µ‡πà ../model/<name>)",
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="‡∏ó‡∏±‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢)",
    )
    args = parser.parse_args()

    # ===== Resolve output_dir =====
    output_dir = _resolve_output_dir(args.name, args.overwrite)
    os.makedirs(output_dir, exist_ok=True)
    print(f"üìÅ Output dir: {output_dir}")

    # ===== Load dataset + tokenizer =====
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, legacy=False, use_fast=False)
    dataset = load_thaisum()

    # ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô train split
    total_train = len(dataset["train"])

    # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á train set ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏∏ --size < 1.0
    if 0 < args.size < 1.0:
        subset_size = int(total_train * args.size)
        print(
            f"‚öôÔ∏è  Using {args.size*100:.0f}% of the training set ‚Üí {subset_size}/{total_train} samples"
        )
        dataset["train"] = dataset["train"].train_test_split(
            test_size=(1 - args.size), seed=42
        )["train"]
    elif args.size == 1.0:
        print(f"‚úÖ Using full training dataset ‚Üí {total_train} samples")
    else:
        print("‚ö†Ô∏è  --size should be in (0,1]; fallback to full dataset.")
        args.size = 1.0

    # ===== Preprocess =====
    tokenized_dataset = preprocess_dataset(dataset, tokenizer)

    # ===== Load model =====
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

    # ===== Data collator =====
    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

    # ===== Detect hardware & precision =====
    use_mps = getattr(torch.backends, "mps", None) and torch.backends.mps.is_available()
    use_cuda = torch.cuda.is_available()
    use_bf16 = (use_cuda and torch.cuda.get_device_capability(0)[0] >= 8) or use_mps

    num_workers = 0 if use_mps else 4
    pin_memory = False if use_mps else True

    extra_args = dict(
        bf16=use_bf16,
        fp16=(use_cuda and not use_bf16),
    )

    device_info = "MPS" if use_mps else ("CUDA" if use_cuda else "CPU")
    print(
        f"üîß Device: {device_info}, Workers: {num_workers}, BF16: {use_bf16}, FP16: {extra_args.get('fp16', False)}"
    )

    # ===== Training arguments =====
    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        num_train_epochs=1,
        per_device_train_batch_size=8,
        learning_rate=5e-5,
        warmup_ratio=0.03,
        lr_scheduler_type="linear",
        eval_strategy="no",
        save_strategy="no",
        logging_steps=100,
        load_best_model_at_end=False,
        dataloader_pin_memory=pin_memory,
        report_to="none",
        seed=42,
        dataloader_num_workers=num_workers,
        **extra_args,
    )

    # ===== Trainer =====
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        data_collator=data_collator,
        processing_class=tokenizer,
    )

    # ===== Train =====
    trainer.train()

    # ===== Save final model =====
    trainer.save_model(training_args.output_dir)
    tokenizer.save_pretrained(training_args.output_dir)
    print("‚úÖ Training finished and model saved.")


if __name__ == "__main__":
    main()
