# üß† Thai Summarization (mT5 + ThaiSum Dataset)

This project implements **Thai text summarization** using **mT5** fine-tuned on the **ThaiSum** dataset. It supports both **fine-tuned** and **zero-shot** evaluation with **ROUGE** and **BERTScore** metrics.

---

## üìÅ Project Structure

```
thai-summarization/
‚îÇ
‚îú‚îÄ‚îÄ data/                        # Evaluation results and logs
‚îú‚îÄ‚îÄ model/                       # Trained models
‚îú‚îÄ‚îÄ notebooks/                   # Jupyter notebooks for analysis
‚îú‚îÄ‚îÄ src/                         # Core source code
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_model.py        # Evaluation (ROUGE, BERTScore)
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py            # Dataset loading and preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ summarize.py             # Summarization inference script
‚îÇ   ‚îî‚îÄ‚îÄ train.py                 # Model fine-tuning
‚îú‚îÄ‚îÄ requirements.txt             # Project dependencies
‚îî‚îÄ‚îÄ README.md                    # Project documentation
```

---

## ‚öôÔ∏è Environment Setup

### ü™ü Windows
```bash
python -m venv .venv
.venv\Scripts\activate     
pip install -r requirements.txt
```

### üçé macOS / üêß Linux
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### üî• Install PyTorch
> ‚ö†Ô∏è **Important:** Install PyTorch **after** installing requirements.txt

#### üñ•Ô∏è Windows / Linux (NVIDIA GPU)
```bash
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126
```

#### üçè macOS (Apple M-series)
```bash
pip3 install torch torchvision
```

---

## üöÄ Training (Fine-tuning mT5)

Fine-tune the `mT5` model on ThaiSum dataset. Supports partial dataset usage via `--size` (number of samples).

```bash
# Train with 1000 samples
python src/train.py --size 1000

# Train with full dataset (no size limit)
python src/train.py
```

üìç **Output:** The trained model will be saved to `./model/FineTuned-{steps}/`

---

## üìä Evaluation (ROUGE + BERTScore)

Evaluate either a fine-tuned model or a zero-shot pre-trained mT5 model.

### ‚úÖ Fine-tuned Model
```bash
# Basic evaluation
python src/evaluate_model.py --model ./model/FineTuned-100 --split test

# With sample limit for faster testing
python src/evaluate_model.py --model ./model/FineTuned-100 --split test --max_samples 500

# Custom output folder name
python src/evaluate_model.py --model ./model/FineTuned-100 --split test --name MyEvalRun
```

### üåè Zero-shot mT5
```bash
# Basic zero-shot evaluation
python src/evaluate_model.py --model google/mt5-small --split test

# With sample limit
python src/evaluate_model.py --model google/mt5-small --split test --max_samples 500
```

### üìÅ Output Files
```
./data/{name}/                   # Default: Results or Results_{timestamp}
‚îú‚îÄ‚îÄ predictions_test.txt         # Model-generated summaries
‚îú‚îÄ‚îÄ references_test.txt          # Ground truth summaries
‚îú‚îÄ‚îÄ inputs_test.txt              # Original input texts
‚îî‚îÄ‚îÄ score/
    ‚îú‚îÄ‚îÄ metrics.json             # Detailed metrics (JSON)
    ‚îî‚îÄ‚îÄ metrics_readable.txt     # Human-readable scores
```

---

## üí¨ Summarize Custom Text

Generate summaries for custom Thai text input.

```bash
# Basic usage
python src/summarize.py --model ./model/FineTuned-100 --text "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ..."

# Zero-shot with pretrained mT5
python src/summarize.py --model google/mt5-small --text "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ..."

# Long text example
python src/summarize.py --model ./model/FineTuned-100 --text "‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£ - ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Å‡∏£‡∏°‡∏≠‡∏∏‡∏ï‡∏∏‡∏ô‡∏¥‡∏¢‡∏°‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ‡∏ù‡∏ô‡∏ü‡πâ‡∏≤‡∏Ñ‡∏∞‡∏ô‡∏≠‡∏á‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏ì‡∏ë‡∏•..."
```

---

## ‚öôÔ∏è Command Arguments

### Training (`train.py`)
| Argument | Description | Default |
|----------|-------------|---------|
| `--fraction` | Fraction of training data to use (0-1) | `1.0` |

### Evaluation (`evaluate_model.py`)
| Argument | Description | Default |
|----------|-------------|---------|
| `--model` | Model path or HF ID | **(required)** |
| `--split` | Dataset split (`validation` / `test`) | `test` |
| `--max_samples` | Limit samples for faster eval | `None` (full) |
| `--batch_size` | Evaluation batch size | `8` |
| `--name` | Output folder name under `./data/` | `Results` |
| `--overwrite_output_dir` | Overwrite existing output folder | `False` |

### Summarize (`summarize.py`)
| Argument | Description | Default |
|----------|-------------|---------|
| `--model` | Model path or HF ID | **(required)** |
| `--text` | Input text to summarize | **(required)** |

---

## üìñ References & Credits

- **Dataset:** [ThaiSum (PyThaiNLP)](https://huggingface.co/datasets/pythainlp/thaisum)
- **Base Model:** [google/mt5-small](https://huggingface.co/google/mt5-small)
- **Metrics:** [Hugging Face Evaluate (ROUGE, BERTScore)](https://huggingface.co/docs/evaluate)

---

<div align="center">

**Author:** Nontapat Chucharnchai  
**Environment:** Python 3.10+, PyTorch, Hugging Face Transformers  
**License:** Research / Academic use only

</div>