import os, sys, argparse
import torch
import numpy as np
from datasets import DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
)
import evaluate

# ===== Import preprocess.py ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞ preprocess dataset =====
sys.path.append(os.path.join(os.path.dirname(os.getcwd()), "src"))
from preprocess import load_thaisum, preprocess_dataset, MODEL_NAME  # noqa: E402


# ===== Helper: ‡πÅ‡∏õ‡∏•‡∏á token ids ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° =====
def decode_text(tokenizer, ids):
    return tokenizer.batch_decode(
        ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
    )


def main():
    # ===== Argument Parser =====
    # ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å command line ‡πÄ‡∏ä‡πà‡∏ô --model, --split, --num_beams ‡∏Ø‡∏•‡∏Ø
    parser = argparse.ArgumentParser(
        description="Evaluate (any) Seq2Seq model on ThaiSum with ROUGE & BERTScore"
    )
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="‡∏û‡∏≤‡∏ò‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÄ‡∏ä‡πà‡∏ô /Model/FineTuned-mT5-ThaiSum ‡∏´‡∏£‡∏∑‡∏≠ google/mt5-small",
    )
    parser.add_argument(
        "--split",
        type=str,
        default="test",
        choices=["validation", "test"],
        help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (default: test)",
    )
    parser.add_argument(
        "--max_samples", type=int, default=None, help="‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô"
    )
    parser.add_argument(
        "--num_beams", type=int, default=4, help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô beam ‡∏ï‡∏≠‡∏ô generate"
    )
    parser.add_argument(
        "--gen_max_len", type=int, default=128, help="‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏µ‡πà generate"
    )
    parser.add_argument(
        "--batch_size", type=int, default=8, help="batch size ‡∏ï‡∏≠‡∏ô evaluate"
    )
    parser.add_argument(
        "--bertscore_model",
        type=str,
        default="xlm-roberta-large",
        help="‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BERTScore",
    )
    parser.add_argument(
        "--input_prefix",
        type=str,
        default="",
        help="‡πÄ‡∏ï‡∏¥‡∏° prefix ‡∏Å‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° input (‡πÄ‡∏ä‡πà‡∏ô 'summarize: ' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö T5/mT5 zero-shot)",
    )
    args = parser.parse_args()

    # ===== ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå (Device) =====
    use_mps = getattr(torch.backends, "mps", None) and torch.backends.mps.is_available()
    use_cuda = torch.cuda.is_available()
    device = "cuda" if use_cuda else ("mps" if use_mps else "cpu")
    print(f"üîß Device: {device}")
    if args.input_prefix:
        print(f"üß© Input prefix: {repr(args.input_prefix)}")

    # ===== ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÇ‡∏ó‡πÄ‡∏Ñ‡πÑ‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå =====
    # ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á tokenizer ‡πÅ‡∏•‡∏∞ model ‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠ path ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô --model
    tokenizer = AutoTokenizer.from_pretrained(args.model, legacy=False, use_fast=False)
    model = AutoModelForSeq2SeqLM.from_pretrained(args.model)
    model.to(device)

    # ===== ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ThaiSum =====
    raw = load_thaisum()
    assert args.split in raw, f"Split '{args.split}' not found in dataset"
    ds = raw[args.split]

    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏∏ --max_samples
    if args.max_samples is not None:
        ds = ds.select(range(min(args.max_samples, len(ds))))

    # ===== ‡πÄ‡∏ï‡∏¥‡∏° prefix (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô zero-shot ‡πÄ‡∏ä‡πà‡∏ô summarize:) =====
    if args.input_prefix:

        def add_prefix(batch):
            batch["body"] = [args.input_prefix + x for x in batch["body"]]
            return batch

        ds = ds.map(add_prefix, batched=True)

    # ===== Tokenize dataset (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô train) =====
    tokenized_dd = preprocess_dataset(DatasetDict({args.split: ds}), tokenizer)
    tokenized = tokenized_dd[args.split]

    # ===== ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Trainer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö evaluate =====
    # ‡πÉ‡∏ä‡πâ Seq2SeqTrainer ‡πÄ‡∏û‡∏∑‡πà‡∏≠ generate summary ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)
    eval_args = Seq2SeqTrainingArguments(
        output_dir="../data/eval_tmp",
        per_device_eval_batch_size=args.batch_size,
        predict_with_generate=True,  # ‡πÉ‡∏´‡πâ generate ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ predict logits
        generation_max_length=args.gen_max_len,
        generation_num_beams=args.num_beams,
        dataloader_pin_memory=(False if use_mps else True),
        report_to="none",  # ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ log ‡πÑ‡∏õ‡∏¢‡∏±‡∏á wandb / tensorboard
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=eval_args,
        data_collator=data_collator,
        eval_dataset=tokenized,
        processing_class=tokenizer,
    )

    # ===== ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• generate ‡∏™‡∏£‡∏∏‡∏õ =====
    print(
        f"üöÄ Generating summaries (beams={args.num_beams}, max_len={args.gen_max_len}) ..."
    )
    preds = trainer.predict(tokenized)

    # ‡πÅ‡∏õ‡∏•‡∏á token id ‚Üí text (‡∏ó‡∏±‡πâ‡∏á pred ‡πÅ‡∏•‡∏∞ reference)
    pred_ids = preds.predictions
    label_ids = preds.label_ids
    label_ids[label_ids == -100] = tokenizer.pad_token_id
    pred_texts = decode_text(tokenizer, pred_ids)
    ref_texts = decode_text(tokenizer, label_ids)

    # ===== ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ROUGE =====
    rouge = evaluate.load("rouge")
    rouge_res = rouge.compute(
        predictions=pred_texts, references=ref_texts, use_stemmer=False
    )

    # ===== ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BERTScore =====
    print(f"üìè Using BERTScore model: {args.bertscore_model}")
    bertscore = evaluate.load("bertscore")
    bs_res = bertscore.compute(
        predictions=pred_texts,
        references=ref_texts,
        model_type=args.bertscore_model,
        device=device,
        batch_size=args.batch_size,
    )
    bs_p = float(np.mean(bs_res["precision"]))
    bs_r = float(np.mean(bs_res["recall"]))
    bs_f1 = float(np.mean(bs_res["f1"]))

    # ===== ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå =====
    def pct(x):
        return round(100 * x, 2)

    print("\nüéØ Results")
    print(f"- ROUGE-1 (F1): {pct(rouge_res['rouge1'])}")
    print(f"- ROUGE-2 (F1): {pct(rouge_res['rouge2'])}")
    print(f"- ROUGE-L (F1): {pct(rouge_res['rougeL'])}")
    print(f"- BERTScore P/R/F1: {pct(bs_p)} / {pct(bs_r)} / {pct(bs_f1)}")

    # ===== ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå =====
    out_dir = (
        os.path.join(args.model, "eval_outputs")
        if os.path.isdir(args.model)
        else "./eval_outputs"
    )
    os.makedirs(out_dir, exist_ok=True)
    split = args.split
    with open(os.path.join(out_dir, f"pred_{split}.txt"), "w", encoding="utf-8") as f:
        for t in pred_texts:
            f.write(t.strip() + "\n")
    with open(os.path.join(out_dir, f"ref_{split}.txt"), "w", encoding="utf-8") as f:
        for t in ref_texts:
            f.write(t.strip() + "\n")
    print(f"\nüìù Saved predictions to: {out_dir}/pred_{split}.txt")
    print(f"üìù Saved references  to: {out_dir}/ref_{split}.txt")


if __name__ == "__main__":
    main()
