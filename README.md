# ğŸ§  Thai Summarization (mT5 + ThaiSum Dataset)

This project implements **Thai text summarization** using **mT5** fine-tuned on the **ThaiSum** dataset. It supports both **fine-tuned** and **zero-shot** evaluation with **ROUGE** and **BERTScore** metrics.

---

## ğŸ“ Project Structure

```
thai-summarization/
â”‚
â”œâ”€â”€ data/                        # Temporary results and evaluation logs
â”œâ”€â”€ Model/                       # Trained models
â”‚   â””â”€â”€ FineTuned-mT5-ThaiSum/   # Fine-tuned model
â”‚       â””â”€â”€ eval_outputs/        # Evaluation outputs (pred/ref)
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for analysis or visualization
â”œâ”€â”€ src/                         # Core source code
â”‚   â”œâ”€â”€ preprocess.py            # Dataset loading and preprocessing
â”‚   â”œâ”€â”€ train.py                 # Model fine-tuning
â”‚   â”œâ”€â”€ evaluate.py              # Evaluation (ROUGE, BERTScore)
â”‚   â””â”€â”€ summarize.py             # Summarization inference script
â”œâ”€â”€ requirements.txt             # Project dependencies
â””â”€â”€ README.md                    # Project documentation
```

---

## âš™ï¸ Environment Setup

### ğŸªŸ Windows
```bash
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
```

### ğŸ macOS / ğŸ§ Linux
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### ğŸ”¥ Install PyTorch

#### ğŸ–¥ï¸ Windows / Linux (NVIDIA GPU)
```bash
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126
```

#### ğŸ macOS (Apple M-series)
```bash
pip3 install torch torchvision
```

---

## ğŸ§© Data Preprocessing

Prepare the ThaiSum dataset for mT5 training and evaluation.

### Steps:
1. **Load the ThaiSum dataset**
   - Keep only `train`, `validation`, `test` splits
   - Remove duplicate `valid` split if present

2. **Sanity check**
   - Print dataset size for each split
   - Show one example row

3. **Tokenization**
   - Convert text into token IDs using `mT5 tokenizer`
   - `body` â†’ `input_ids`, `attention_mask`
   - `summary` â†’ `labels`

4. **Truncation**
   - Input (body) â‰¤ 512 tokens
   - Target (summary) â‰¤ 128 tokens

5. **Remove unnecessary columns**
   - Drop `title`, `body`, `summary`, `tags`, `url`, `type`
   - Keep only `input_ids`, `attention_mask`, `labels`

### ğŸ“˜ Run preprocessing:
```bash
python src/preprocess.py
```

---

## ğŸš€ Training (Fine-tuning mT5)

Fine-tune the `mT5` model on ThaiSum dataset. Supports partial dataset usage via `--fraction` (0â€“1).

```bash
# Example: train with 30% of the training set
python -u "src/train.py" --fraction 0.3
```

ğŸ“ **Output:** The trained model will be saved to `Model/FineTuned-mT5-ThaiSum/`

---

## ğŸ“Š Evaluation (ROUGE + BERTScore)

Evaluate either a fine-tuned model or a zero-shot pre-trained mT5 model.

### âœ… Fine-tuned Model
```bash
python -u src/evaluate.py \
  --model /Model/FineTuned-mT5-ThaiSum \
  --split test
```

### ğŸŒ Zero-shot mT5
```bash
python -u src/evaluate.py \
  --model google/mt5-small \
  --input_prefix "summarize: " \
  --split test
```

### ğŸ“ Output Files
```
Model/FineTuned-mT5-ThaiSum/eval_outputs/
â”œâ”€â”€ pred_test.txt   # Model-generated summaries
â””â”€â”€ ref_test.txt    # Reference summaries (ground truth)
```

---

## ğŸ”„ Evaluation Process

The evaluation pipeline uses **ROUGE-1/2/L (F1)** and **BERTScore** metrics to assess model quality.

### Process Steps:
1. **Load model and tokenizer** (path or Hugging Face ID via `--model`)
2. **Load ThaiSum dataset split** (`validation` / `test`)
3. **Add input prefix** (optional, e.g., `"summarize: "` for zero-shot)
4. **Preprocess and tokenize dataset**
5. **Run generation** using beam search
6. **Decode predictions** â†’ `pred_texts` & `ref_texts`
7. **Compute metrics** (ROUGE & BERTScore)
8. **Report and save results**

---

## âš™ï¸ Evaluation Arguments

| Argument | Description | Example |
|----------|-------------|---------|
| `--model` | Model path or Hugging Face ID | `--model /Model/FineTuned-mT5-ThaiSum`<br>`--model google/mt5-small` |
| `--split` | Dataset split (`validation` or `test`) | `--split test` |
| `--max_samples` | Limit samples for faster evaluation | `--max_samples 500` |
| `--num_beams` | Beam search size | `--num_beams 4` |
| `--gen_max_len` | Maximum generation length (tokens) | `--gen_max_len 128` |
| `--batch_size` | Evaluation batch size | `--batch_size 8` |
| `--bertscore_model` | Model for BERTScore computation | `--bertscore_model xlm-roberta-large` |
| `--input_prefix` | Prefix for zero-shot summarization | `--input_prefix "summarize: "` |

---

## ğŸ§¹ Additional Commands

```bash
# Clear pip cache
pip cache purge

# Run preprocessing (dataset check)
python src/preprocess.py
```

---

## ğŸ’¡ Performance Tips

- **Training:** Use `--fraction` to fine-tune on a percentage of the dataset
- **Inference:** Larger `--num_beams` â†’ higher quality but slower inference
- **BERTScore:** `xlm-roberta-large` performs best for Thai text
- **Hardware:** On macOS M-series, `mps` backend runs much faster than CPU
- **Storage:** Temporary logs in `data/eval_tmp/` can be safely deleted

---

## ğŸ“– References & Credits

- **Dataset:** [ThaiSum (PyThaiNLP)](https://huggingface.co/datasets/pythainlp/thaisum)
- **Base Model:** [google/mt5-small](https://huggingface.co/google/mt5-small)
- **Metrics:** [Hugging Face Evaluate (ROUGE, BERTScore)](https://huggingface.co/docs/evaluate)

---

<div align="center">

**Author:** Nontapat Chucharnchai  
**Environment:** Python 3.10+, PyTorch, Hugging Face Transformers  
**License:** Research / Academic use only

</div>